# -*- coding: utf-8 -*-
"""ORIGINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19bn0bUnmyRgQyqd7_78cihf_uBrhRzTS
"""

import os
import cv2
import numpy as np
import json
import tensorflow as tf
from tensorflow.keras import layers, models, mixed_precision, activations
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard
from tensorflow.keras.metrics import TopKCategoricalAccuracy
from tensorflow.keras.utils import Sequence, plot_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from contextlib import contextmanager
import psutil
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive, files
import gc
import time
from datetime import datetime
import warnings
import glob
import pickle
warnings.filterwarnings('ignore')

# Enable mixed precision for faster training and lower memory usage
mixed_precision.set_global_policy('mixed_float16')

# XLA (JIT)
tf.config.optimizer.set_jit(True)

print("Mounting Google Drive...")
drive.mount('/content/drive')

# =============================================================================
# CONFIGURATION
# =============================================================================
CONFIG = {
    'VIDEO_DIR': "/content/drive/MyDrive/videos",
    'ANNOTATION_FILE': "/content/drive/MyDrive/TFG/media/annotations.json",
    'CHECKPOINT_DIR': "/content/drive/MyDrive/checkpoints_tryfinal",
    'LOGS_DIR': "/content/drive/MyDrive/logs_tryfinal",
    'RESULTS_DIR': "/content/drive/MyDrive/results_tryfinal",

    # Video processing parameters
    'IMG_HEIGHT': 112,
    'IMG_WIDTH': 112,
    'NUM_FRAMES': 16,
    'FPS_TARGET': 8,

    # Training parameters
    'BATCH_SIZE': 8,
    'EPOCHS': 100,
    'LEARNING_RATE': 5e-4,
    'PATIENCE': 40,
    'MIN_DELTA': 1e-4,

    # Data parameters
    'TEST_SIZE': 0.10,
    'VAL_SIZE': 0.15,
    'MIN_DURATION': 1.0,
    'MAX_SEGMENTS_PER_VIDEO': 20,

    # Augmentation parameters
    'TEMPORAL_SHIFT': 0.25,
    'BRIGHTNESS_RANGE': 0.20,
    'CONTRAST_RANGE': 0.20,
    'ROTATION_RANGE': 15,
    'ZOOM_RANGE': 0.15,
    'AUGMENTATION_PROB': 0.8,
}

# Load unique classes from labels
CLASSES = [
    "Eating", "Cooking", "Exercising", "Walking", "Using_phone", "Vacuum", "Tidying",
    "Gaming", "Snacking", "Reading", "Standing", "Drinking", "Sitting", "Doing_the_dishes",
    "Watching_TV", "Sleeping", "Doing_the_laundry", "Working", "Cleaning", "Playing"
]

global NUM_CLASSES

# Create directories
for dir_path in [CONFIG['CHECKPOINT_DIR'], CONFIG['LOGS_DIR'], CONFIG['RESULTS_DIR']]:
    os.makedirs(dir_path, exist_ok=True)

print(f"Target: {CONFIG['NUM_FRAMES']} frames at {CONFIG['IMG_HEIGHT']}x{CONFIG['IMG_WIDTH']}")

# =============================================================================
# MODEL LOADING FUNCTIONALITY
# =============================================================================
def find_latest_model():
    """Find the most recent model in the checkpoints directory"""
    checkpoint_pattern = os.path.join(CONFIG['CHECKPOINT_DIR'], 'best_model_*.keras')
    model_files = glob.glob(checkpoint_pattern)

    if not model_files:
        return None, None, 0

    # Sort by modification time (most recent first)
    model_files.sort(key=os.path.getmtime, reverse=True)
    latest_model = model_files[0]

    # Try to find corresponding label encoder
    model_timestamp = latest_model.split('_')[-1].replace('.keras', '')
    encoder_pattern = os.path.join(CONFIG['CHECKPOINT_DIR'], f'label_encoder_{model_timestamp}.pkl')

    label_encoder = None
    if os.path.exists(encoder_pattern):
        with open(encoder_pattern, 'rb') as f:
            label_encoder = pickle.load(f)

    epoch_count = len(model_files)

    return latest_model, label_encoder, epoch_count

def load_existing_model():
    """Load existing model if available"""
    latest_model_path, label_encoder, model_count = find_latest_model()

    if latest_model_path:
        print(f"Found {model_count} existing model(s)")
        print(f"Loading model: {os.path.basename(latest_model_path)}")

        try:
            model = tf.keras.models.load_model(latest_model_path)
            print(f"Successfully loaded model {model_count}")
            return model, label_encoder, model_count
        except Exception as e:
            print(f"Failed to load model: {e}")
            print("Will train from scratch")
            return None, None, 0
    else:
        print("No existing models found. Starting fresh training.")
        return None, None, 0

# =============================================================================
# MEMORY MONITORING UTILITY
# =============================================================================
@contextmanager
def memory_limit_check(threshold=0.85):
    """Context manager to monitor memory usage"""
    initial_memory = psutil.virtual_memory().percent
    try:
        yield
    finally:
        current_memory = psutil.virtual_memory().percent
        if current_memory > threshold * 100:
            print(f"High memory usage: {current_memory:.1f}%")
            gc.collect()

class VideoDataProcessor:
    """Video data processor with memory optimization"""

    def __init__(self, config):
        self.config = config

    def extract_video_segments(self, video_path, annotations):
        """Extract video segments based on annotations"""
        segments = []

        for result in annotations:
            start_time = result['value']['start']
            end_time = result['value']['end']
            label = result['value']['labels'][0]

            # Skip short segments and NoAction
            duration = end_time - start_time
            if duration < self.config['MIN_DURATION'] or label == "NoAction":
                continue

            segments.append({
                'video_path': video_path,
                'start_time': start_time,
                'end_time': end_time,
                'duration': duration,
                'label': label
            })

        return segments

    def sample_frames_from_segment(self, video_path, start_time, end_time, num_frames):
        """Sample frames from a specific time segment"""
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            print(f"Error opening video: {video_path}")
            return None

        try:
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            if fps <= 0 or total_frames <= 0:
                print(f"Invalid video properties: {video_path}")
                cap.release()
                return None

            # Convert time to frame indices
            start_frame = int(start_time * fps)
            end_frame = int(end_time * fps)

            # Ensure I don't exceed video bounds
            start_frame = max(0, start_frame)
            end_frame = min(total_frames - 1, end_frame)

            segment_frames = end_frame - start_frame

            if segment_frames <= 0:
                print(f"Invalid segment: {video_path}")
                cap.release()
                return None

            if segment_frames < num_frames:
                # If segment is too short, repeat frames
                frame_indices = np.linspace(start_frame, end_frame, num_frames, dtype=int)
            else:
                # Sample frames uniformly
                frame_indices = np.linspace(start_frame, end_frame, num_frames, dtype=int)

            frames = []
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()

                if not ret:
                    # If frame reading fails, use the last valid frame or create a black frame
                    if frames:
                        frame = frames[-1].copy()
                    else:
                        frame = np.zeros((self.config['IMG_HEIGHT'], self.config['IMG_WIDTH'], 3), dtype=np.uint8)

                # Resize and normalize
                frame = cv2.resize(frame, (self.config['IMG_WIDTH'], self.config['IMG_HEIGHT']))
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)

            cap.release()
            return np.array(frames, dtype=np.float32) / 255.0

        except Exception as e:
            print(f"Error processing segment from {video_path}: {e}")
            cap.release()
            return None

def balance_classes(segments, min_samples=5, max_samples=None):
    """Balance classes using multiple strategies"""
    from collections import defaultdict
    import random

    print("Balancing classes...")

    # Group segments by class
    class_segments = defaultdict(list)
    for segment in segments:
        class_segments[segment['label']].append(segment)

    # Print initial distribution
    print("Original class distribution:")
    for class_name, class_segs in sorted(class_segments.items()):
        print(f"  {class_name}: {len(class_segs)} segments")

    # Remove classes with too few samples
    classes_to_remove = []
    for class_name, class_segs in class_segments.items():
        if len(class_segs) < min_samples:
            classes_to_remove.append(class_name)

    for class_name in classes_to_remove:
        print(f"Removing class '{class_name}' (only {len(class_segments[class_name])} samples)")
        del class_segments[class_name]

    global NUM_CLASSES
    NUM_CLASSES = len(class_segments)

    if not class_segments:
        raise ValueError("No classes left after filtering!")

    # Calculate target number of samples per class
    class_counts = [len(segs) for segs in class_segments.values()]

    if max_samples is None:
        # Use median as target to balance between over and under-represented classes
        target_samples = int(np.median(class_counts))
        # Ensure minimum viable samples for train/val/test split
        target_samples = max(target_samples, min_samples * 2)
    else:
        target_samples = max_samples

    print(f"Target samples per class: {target_samples}")

    balanced_segments = []

    for class_name, class_segs in class_segments.items():
        current_count = len(class_segs)

        if current_count >= target_samples:
            # Downsample: randomly select target_samples
            selected_segments = random.sample(class_segs, target_samples)
        else:
            # Upsample: create augmented versions
            selected_segments = class_segs.copy()

            # Calculate how many augmented samples we need
            needed_samples = target_samples - current_count

            # Create augmented samples by slightly modifying existing ones
            for i in range(needed_samples):
                # Select a random segment to augment
                base_segment = random.choice(class_segs)

                # Create augmented version with slight temporal modifications
                augmented_segment = base_segment.copy()

                # Add slight random variations to start/end times (±10%)
                duration = base_segment['end_time'] - base_segment['start_time']
                time_variation = duration * 0.1

                # Ensure we don't go outside video bounds
                new_start = max(0, base_segment['start_time'] + random.uniform(-time_variation, time_variation))
                new_end = new_start + duration

                augmented_segment.update({
                    'start_time': new_start,
                    'end_time': new_end,
                    'augmented': True  # Mark as augmented
                })

                selected_segments.append(augmented_segment)

        balanced_segments.extend(selected_segments)
        print(f"  {class_name}: {current_count} → {len(selected_segments)} segments")

    # Shuffle the balanced segments
    random.shuffle(balanced_segments)

    print(f"Balanced dataset: {len(balanced_segments)} total segments")
    return balanced_segments

def load_and_process_annotations(video_dir, annotation_file):
    """Load and process annotations with class balancing"""
    print("Loading annotations...")

    with open(annotation_file, 'r') as f:
        annotations = json.load(f)

    processor = VideoDataProcessor(CONFIG)
    all_segments = []
    failed_videos = []

    for ann in annotations:
        try:
            # Extract video path
            video_url = ann['data']['video_url']
            video_name = video_url.split('videos/')[-1]
            video_path = os.path.join(video_dir, video_name)

            if not os.path.exists(video_path):
                failed_videos.append(video_name)
                continue

            # Extract segments from this video
            results = ann['annotations'][0]['result']
            segments = processor.extract_video_segments(video_path, results)

            # Limit segments per video to avoid memory issues
            if len(segments) > CONFIG['MAX_SEGMENTS_PER_VIDEO']:
                # Sort by duration and take longest segments
                segments.sort(key=lambda x: x['duration'], reverse=True)
                segments = segments[:CONFIG['MAX_SEGMENTS_PER_VIDEO']]

            all_segments.extend(segments)

        except Exception as e:
            print(f"Error processing annotation: {e}")
            continue

    print(f"Loaded {len(all_segments)} segments from {len(annotations) - len(failed_videos)} videos")
    if failed_videos:
        print(f"Failed to load {len(failed_videos)} videos")

    # Balance classes
    balanced_segments = balance_classes(all_segments, min_samples=5)

    return balanced_segments

# =============================================================================
# DATA GENERATOR
# =============================================================================
class VideoDataGenerator(Sequence):
    """
    Memory-efficient video data generator with consistent batch sizes
    """

    def __init__(self, segments, label_encoder, config, is_training=True,
                 augment=True, shuffle=True):
        self.segments = segments
        self.label_encoder = label_encoder
        self.config = config
        self.is_training = is_training
        self.augment = augment and is_training
        self.shuffle = shuffle
        self.processor = VideoDataProcessor(config)

        # Pre-filter segments to ensure we have valid batches
        self.valid_segments = self._filter_valid_segments()

        # Calculate number of complete batches we can make
        self.num_complete_batches = len(self.valid_segments) // self.config['BATCH_SIZE']

        # Trim segments to have only complete batches
        self.segments_for_batches = self.valid_segments[:self.num_complete_batches * self.config['BATCH_SIZE']]

        print(f"Generator initialized: {len(self.segments_for_batches)} segments -> {self.num_complete_batches} batches")

        self.on_epoch_end()

    def _filter_valid_segments(self):
        """Pre-filter segments to remove those that might cause issues"""
        valid_segments = []

        for segment in self.segments:
            try:
                # Quick validation - check if video exists and has reasonable duration
                if os.path.exists(segment['video_path']):
                    duration = segment['end_time'] - segment['start_time']
                    if duration >= self.config['MIN_DURATION']:
                        valid_segments.append(segment)
            except Exception as e:
                print(f"Skipping invalid segment: {e}")
                continue

        return valid_segments

    def __len__(self):
        """Return number of complete batches"""
        return self.num_complete_batches

    def __getitem__(self, index):
        """Get a batch of data - always returns exactly BATCH_SIZE samples"""
        # Calculate batch indices
        start_idx = index * self.config['BATCH_SIZE']
        end_idx = start_idx + self.config['BATCH_SIZE']

        batch_segments = self.segments_for_batches[start_idx:end_idx]

        batch_x = []
        batch_y = []

        # Process each segment in the batch
        for segment in batch_segments:
            frames = None
            max_retries = 3

            for retry in range(max_retries):
                try:
                    frames = self.processor.sample_frames_from_segment(
                        segment['video_path'],
                        segment['start_time'],
                        segment['end_time'],
                        self.config['NUM_FRAMES']
                    )

                    if frames is not None:
                        break

                except Exception as e:
                    print(f"Retry {retry+1}/{max_retries} for segment: {e}")
                    continue

            # If I still don't have frames, create a dummy frame sequence
            if frames is None:
                print(f"Creating dummy frames for failed segment")
                frames = np.zeros((self.config['NUM_FRAMES'],
                                 self.config['IMG_HEIGHT'],
                                 self.config['IMG_WIDTH'], 3),
                                dtype=np.float32)

            # Apply augmentation if enabled
            if self.augment:
                frames = apply_augmentation(frames, self.config)

            batch_x.append(frames)

            # Encode label
            try:
                label = self.label_encoder.transform([segment['label']])[0]
            except ValueError:
                # If label not found, use first class as default
                label = 0
                print(f"Unknown label '{segment['label']}', using default class 0")

            batch_y.append(label)

        # Ensure I have exactly BATCH_SIZE samples
        while len(batch_x) < self.config['BATCH_SIZE']:
            # Duplicate the last sample if needed
            if batch_x:
                batch_x.append(batch_x[-1])
                batch_y.append(batch_y[-1])
            else:
                # Create dummy data if batch is completely empty
                dummy_frames = np.zeros((self.config['NUM_FRAMES'],
                                       self.config['IMG_HEIGHT'],
                                       self.config['IMG_WIDTH'], 3),
                                      dtype=np.float32)
                batch_x.append(dummy_frames)
                batch_y.append(0)

        # Convert to numpy arrays
        batch_x = np.array(batch_x[:self.config['BATCH_SIZE']], dtype=np.float32)
        batch_y = tf.keras.utils.to_categorical(batch_y[:self.config['BATCH_SIZE']],
                                              num_classes=NUM_CLASSES)

        return batch_x, batch_y

    def on_epoch_end(self):
        """Called at the end of each epoch"""
        if self.shuffle:
            np.random.shuffle(self.segments_for_batches)

# =============================================================================
# AUGMENTATION TO COMBAT OVERFITTING
# =============================================================================
def apply_augmentation(frames, config):
    """
    Apply augmentation to combat overfitting
    """
    # Only apply augmentation with specified probability
    if np.random.random() > config.get('AUGMENTATION_PROB', 0.8):
        return frames

    # Temporal augmentations
    if np.random.random() < 0.6:
        # Temporal shift
        shift = int(config['TEMPORAL_SHIFT'] * config['NUM_FRAMES'])
        if shift > 0:
            frames = np.roll(frames, np.random.randint(-shift, shift+1), axis=0)

    if np.random.random() < 0.4:  # Temporal dropout
        # Randomly zero out some frames
        num_zeros = np.random.randint(1, min(4, config['NUM_FRAMES']//4))
        zero_indices = np.random.choice(config['NUM_FRAMES'], num_zeros, replace=False)
        frames[zero_indices] = 0

    if np.random.random() < 0.5:  # Frame repetition
        # Repeat some frames to simulate different speeds
        repeat_idx = np.random.randint(0, config['NUM_FRAMES'])
        frames[repeat_idx] = frames[max(0, repeat_idx-1)]

    # Spatial augmentations
    if np.random.random() < 0.7:
        # Brightness adjustment
        brightness_factor = 1.0 + np.random.uniform(-config['BRIGHTNESS_RANGE'],
                                                   config['BRIGHTNESS_RANGE'])
        frames = np.clip(frames * brightness_factor, 0, 1)

    if np.random.random() < 0.7:
        # Contrast adjustment
        contrast_factor = 1.0 + np.random.uniform(-config['CONTRAST_RANGE'],
                                                config['CONTRAST_RANGE'])
        frames = np.clip((frames - 0.5) * contrast_factor + 0.5, 0, 1)

    if np.random.random() < 0.6:  # Horizontal flip
        frames = frames[:, :, ::-1, :]

    if np.random.random() < 0.3:  # Gaussian noise
        noise_std = np.random.uniform(0.005, 0.02)
        noise = np.random.normal(0, noise_std, frames.shape)
        frames = np.clip(frames + noise, 0, 1)

    if np.random.random() < 0.4:  # Random erasing
        # Randomly erase rectangular patches
        for _ in range(np.random.randint(1, 3)):
            h, w = frames.shape[1], frames.shape[2]
            patch_h = np.random.randint(h//8, h//4)
            patch_w = np.random.randint(w//8, w//4)
            y = np.random.randint(0, h - patch_h)
            x = np.random.randint(0, w - patch_w)
            frames[:, y:y+patch_h, x:x+patch_w, :] = np.random.random()

    if np.random.random() < 0.3:  # Cutout (set patches to 0)
        h, w = frames.shape[1], frames.shape[2]
        patch_h = np.random.randint(h//10, h//6)
        patch_w = np.random.randint(w//10, w//6)
        y = np.random.randint(0, h - patch_h)
        x = np.random.randint(0, w - patch_w)
        frames[:, y:y+patch_h, x:x+patch_w, :] = 0

    return frames.astype(np.float32)

# =============================================================================
# 3D CNN ARCHITECTURE
# =============================================================================
def build_3dcnn_model():
    """Build the 3D CNN model"""
    inputs = layers.Input(shape=(CONFIG['NUM_FRAMES'], CONFIG['IMG_HEIGHT'],
                                CONFIG['IMG_WIDTH'], 3), name='video_input')

    # Initial stem with depthwise separable convolutions for efficiency
    x = layers.Conv3D(32, (3, 7, 7), strides=(1, 2, 2), padding='same',
                     name='stem_conv')(inputs)
    x = layers.BatchNormalization(name='stem_bn')(x)
    x = layers.Activation('relu', name='stem_relu')(x)

    def cnn_3d_block(x, filters, strides=(1, 1, 1), expansion=4, name_prefix=''):
        """3D block with depthwise separable convolutions"""
        input_filters = x.shape[-1]

        # Expansion
        expanded = layers.Conv3D(filters * expansion, (1, 1, 1), padding='same',
                               name=f'{name_prefix}_expand_conv')(x)
        expanded = layers.BatchNormalization(name=f'{name_prefix}_expand_bn')(expanded)
        expanded = layers.Activation('relu', name=f'{name_prefix}_expand_relu')(expanded)

        dw = layers.Conv3D(filters * expansion, (3, 3, 3), strides=strides,
                          padding='same',
                          groups=filters * expansion if strides == (1, 1, 1) else 1,
                          name=f'{name_prefix}_dw_conv')(expanded)

        dw = layers.BatchNormalization(name=f'{name_prefix}_dw_bn')(dw)
        dw = layers.Activation('relu', name=f'{name_prefix}_dw_relu')(dw)

        # Projection
        output = layers.Conv3D(filters, (1, 1, 1), padding='same',
                             name=f'{name_prefix}_project_conv')(dw)
        output = layers.BatchNormalization(name=f'{name_prefix}_project_bn')(output)

        # Residual connection
        if strides == (1, 1, 1) and input_filters == filters:
            output = layers.Add(name=f'{name_prefix}_add')([x, output])

        return output

    # Progressive feature extraction
    x = cnn_3d_block(x, 64, strides=(1, 2, 2), name_prefix='block1_1')
    x = cnn_3d_block(x, 64, name_prefix='block1_2')

    x = cnn_3d_block(x, 128, strides=(2, 2, 2), name_prefix='block2_1')
    x = cnn_3d_block(x, 128, name_prefix='block2_2')
    x = layers.Dropout(0.1, name='dropout_1')(x)

    x = cnn_3d_block(x, 256, strides=(2, 2, 2), name_prefix='block3_1')
    x = cnn_3d_block(x, 256, name_prefix='block3_2')
    x = layers.Dropout(0.15, name='dropout_2')(x)

    # Attention mechanism (Squeeze-and-Excitation)
    se = layers.GlobalAveragePooling3D(name='se_gap')(x)
    se = layers.Dense(256 // 16, activation='relu', name='se_dense1')(se)
    se = layers.Dense(256, activation='sigmoid', name='se_dense2')(se)
    se = layers.Reshape((1, 1, 1, 256), name='se_reshape')(se)
    x = layers.Multiply(name='se_multiply')([x, se])

    # Global pooling and classification
    x = layers.GlobalAveragePooling3D(name='global_pool')(x)
    x = layers.Dense(512, activation='relu', name='fc1')(x)
    x = layers.Dropout(0.5, name='dropout_final')(x)
    x = layers.Dense(256, activation='relu', name='fc2')(x)
    x = layers.Dropout(0.3, name='dropout_final2')(x)

    # Output layer with proper dtype for mixed precision
    outputs = layers.Dense(NUM_CLASSES, activation='softmax', dtype='float32',
                          name='predictions')(x)

    model = models.Model(inputs, outputs, name='3DCNN')

    # Compile with proper metrics
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=CONFIG['LEARNING_RATE'],
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-7
    )
    optimizer = mixed_precision.LossScaleOptimizer(optimizer)

    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=[
            'accuracy',
            TopKCategoricalAccuracy(k=3, name='top_3_accuracy'),
            TopKCategoricalAccuracy(k=5, name='top_5_accuracy')
        ]
    )

    return model

# =============================================================================
# CALLBACKS
# =============================================================================
def create_callbacks():
    """Create the callbacks that the training will use"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Conservative learning rate schedule
    def lr_schedule(epoch, lr):
        """Modify learning rate depending on the epoch that we are"""
        if epoch < 15:
            return CONFIG['LEARNING_RATE'] * (epoch + 1) / 15
        elif epoch < 40:
            return CONFIG['LEARNING_RATE']
        elif epoch < 70:
            return CONFIG['LEARNING_RATE'] * 0.5
        else:
            return CONFIG['LEARNING_RATE'] * 0.1

    callbacks = [
        # Model checkpointing
        ModelCheckpoint(
            filepath=os.path.join(CONFIG['CHECKPOINT_DIR'], f'best_model_{timestamp}.keras'),
            monitor='val_accuracy',
            save_best_only=True,
            save_weights_only=False,
            mode='max',
            verbose=1,
            save_freq='epoch'
        ),

        # Early stopping to prevent overfitting
        EarlyStopping(
            monitor='val_accuracy',
            patience=CONFIG['PATIENCE'],
            min_delta=CONFIG['MIN_DELTA'],
            restore_best_weights=True,
            verbose=1,
            mode='max'
        ),

        # Conservative learning rate schedule
        tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1),

        # Reduce LR on plateau for validation accuracy
        ReduceLROnPlateau(
            monitor='val_accuracy',
            factor=0.8,
            patience=10,
            min_lr=1e-7,
            verbose=1,
            mode='max'
        ),

        # TensorBoard
        TensorBoard(
            log_dir=os.path.join(CONFIG['LOGS_DIR'], f'training_{timestamp}'),
            histogram_freq=1,
            write_graph=True,
            write_images=False,
            update_freq='epoch'
        )
    ]

    return callbacks

# =============================================================================
# VISUALIZATION
# =============================================================================
def plot_training_history(history, save_path):
    """Training visualizations"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # Accuracy
    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # Loss
    axes[0, 1].plot(history.history['loss'], label='Training Loss', linewidth=2)
    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # Top-3 Accuracy
    axes[0, 2].plot(history.history['top_3_accuracy'], label='Training Top-3', linewidth=2)
    axes[0, 2].plot(history.history['val_top_3_accuracy'], label='Validation Top-3', linewidth=2)
    axes[0, 2].set_title('Top-3 Accuracy', fontsize=14, fontweight='bold')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Top-3 Accuracy')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # Learning Rate
    if 'lr' in history.history:
        axes[1, 0].plot(history.history['lr'], linewidth=2, color='red')
        axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)

    # Training vs Validation Loss Difference
    loss_diff = np.array(history.history['val_loss']) - np.array(history.history['loss'])
    axes[1, 1].plot(loss_diff, linewidth=2, color='purple')
    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
    axes[1, 1].set_title('Overfitting Monitor (Val Loss - Train Loss)', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss Difference')
    axes[1, 1].grid(True, alpha=0.3)

    # Top-5 Accuracy if available
    if 'top_5_accuracy' in history.history:
        axes[1, 2].plot(history.history['top_5_accuracy'], label='Training Top-5', linewidth=2)
        axes[1, 2].plot(history.history['val_top_5_accuracy'], label='Validation Top-5', linewidth=2)
        axes[1, 2].set_title('Top-5 Accuracy', fontsize=14, fontweight='bold')
        axes[1, 2].set_xlabel('Epoch')
        axes[1, 2].set_ylabel('Top-5 Accuracy')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()

    # Save training metrics to JSON for later analysis
    metrics_path = save_path.replace('.png', '_metrics.json')
    with open(metrics_path, 'w') as f:
        # Convert numpy arrays to lists for JSON serialization
        history_dict = {}
        for key, values in history.history.items():
            history_dict[key] = [float(v) for v in values]
        json.dump(history_dict, f, indent=2)

    print(f"Training visualizations saved to: {save_path}")
    print(f"Training metrics saved to: {metrics_path}")

# =============================================================================
# EVALUATION
# =============================================================================
def evaluate_model(model, test_generator, label_encoder):
    """Model evaluation with visualizations"""
    print("Evaluating model...")

    # Predict on test set
    y_pred = []
    y_true = []
    y_pred_proba = []

    print("Generating predictions...")
    for i in range(len(test_generator)):
        batch_x, batch_y = test_generator[i]
        if batch_x.shape[0] > 0:
            predictions = model.predict(batch_x, verbose=0)
            y_pred_proba.extend(predictions)
            y_pred.extend(np.argmax(predictions, axis=1))
            y_true.extend(np.argmax(batch_y, axis=1))

    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    y_pred_proba = np.array(y_pred_proba)

    # Get class names
    class_names = label_encoder.classes_

    # Accuracy & metrics
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report

    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, average=None, labels=range(len(class_names))
    )

    print("Classification Results:")
    print(f"Overall Accuracy: {accuracy:.4f}")
    print("\nPer-class Results:")
    for i, class_name in enumerate(class_names):
        if i < len(precision):
            print(f"{class_name:20}: Precision={precision[i]:.3f}, Recall={recall[i]:.3f}, F1={f1[i]:.3f}, Support={support[i]}")

    # Detailed classification report
    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)
    print("\n" + classification_report(y_true, y_pred, target_names=class_names, zero_division=0))

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    cm_norm = confusion_matrix(y_true, y_pred, normalize='true')

    fig, axes = plt.subplots(1, 2, figsize=(20, 8))

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names,
                yticklabels=class_names, ax=axes[0])
    axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Predicted')
    axes[0].set_ylabel('True')

    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens', xticklabels=class_names,
                yticklabels=class_names, ax=axes[1])
    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Predicted')
    axes[1].set_ylabel('True')

    plt.tight_layout()
    save_path = os.path.join(CONFIG['RESULTS_DIR'], 'confusion_matrix.png')
    plt.savefig(save_path, dpi=300)
    plt.show()
    print(f"Confusion matrix saved to: {save_path}")

    return report, cm


def generate_qualitative_analysis_from_model(model, test_generator, label_encoder, output_filename='qualitative_analysis_live.png'):
    """
    Evaluates the model in the test set to find multiple examples of correct and incorrect
    classifications, showing them in to separate grids.
    """
    NUM_CORRECT_NEEDED = 9
    NUM_INCORRECT_NEEDED = 12

    print(f"Searching for {NUM_CORRECT_NEEDED} hits and {NUM_INCORRECT_NEEDED} fails in the test set...")

    correct_examples = []
    incorrect_examples = []

    # Iterates through the generator until it finds enough examples
    for i in range(len(test_generator)):
        if len(correct_examples) >= NUM_CORRECT_NEEDED and len(incorrect_examples) >= NUM_INCORRECT_NEEDED:
            break

        batch_x, batch_y = test_generator[i]
        predictions = model.predict(batch_x, verbose=0)
        predicted_labels_int = np.argmax(predictions, axis=1)
        true_labels_int = np.argmax(batch_y, axis=1)

        for j in range(len(true_labels_int)):
            # Checks if it has found enough frames of each type
            found_all_correct = len(correct_examples) >= NUM_CORRECT_NEEDED
            found_all_incorrect = len(incorrect_examples) >= NUM_INCORRECT_NEEDED

            true_label_name = label_encoder.inverse_transform([true_labels_int[j]])[0]
            predicted_label_name = label_encoder.inverse_transform([predicted_labels_int[j]])[0]
            frame_to_display = batch_x[j][CONFIG['NUM_FRAMES'] // 2]

            example_data = {
                "frame": frame_to_display,
                "true_label": true_label_name,
                "predicted_label": predicted_label_name
            }

            # Append to the CORRECT list
            if true_labels_int[j] == predicted_labels_int[j] and not found_all_correct:
                correct_examples.append(example_data)

            # Append to the INCORRECT list
            elif true_labels_int[j] != predicted_labels_int[j] and not found_all_incorrect:
                incorrect_examples.append(example_data)

    print(f"Hits found: {len(correct_examples)}/{NUM_CORRECT_NEEDED}")
    print(f"Fails found: {len(incorrect_examples)}/{NUM_INCORRECT_NEEDED}")

    # Generate the grids
    if correct_examples:
        _plot_image_grid(
            examples=correct_examples,
            grid_dims=(3, 3), # 3 rows, 3 columns for 9 images
            title="Classification hits from the model",
            filename="correct_classifications_grid.png"
        )
    else:
        print("No hits were found to visualize.")

    if incorrect_examples:
        _plot_image_grid(
            examples=incorrect_examples,
            grid_dims=(4, 3), # 4 rows, 3 columns for 12 images
            title="Classification fails from the model",
            filename="incorrect_classifications_grid.png"
        )
    else:
        print("No fails were found to visualize.")


def _plot_image_grid(examples, grid_dims, title, filename):
    """
    Auxiliary function to create and store a grid of images with their labels.
    """
    rows, cols = grid_dims

    # Adjust the space
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4.5))

    # Ensure that the axes are always an array, even with only one row/column
    if rows == 1 and cols == 1:
        axes = [axes]
    elif rows == 1 or cols == 1:
        axes = axes.flatten()
    else:
        axes = axes.flatten()

    for i, example in enumerate(examples):
        # Shows the frame
        axes[i].imshow(example["frame"])

        # Set the label color
        is_correct = (example["predicted_label"] == example["true_label"])
        color = "green" if is_correct else "red"

        # Create the title for every frame
        axes[i].set_title(
            f"Pred: {example['predicted_label']}\nReal: {example['true_label']}",
            fontsize=11,
            color=color,
            pad=15
        )
        axes[i].axis('off')

    # Hide the spare axes if there are less images than cells in the grid
    for i in range(len(examples), len(axes)):
        axes[i].axis('off')

    plt.suptitle(title, fontsize=20, fontweight='bold')

    plt.tight_layout(
        rect=[0, 0.03, 1, 0.95],  # [left, bottom, right, top]
        h_pad=5.0,  # Vertical space between frames
        w_pad=1.0   # Horizontal space between frames
    )

    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"Analysis grid stored as '{filename}'")



# =============================================================================
# FUNCIÓN PARA VISUALIZAR LA ARQUITECTURA COMPLETA DEL MODELO
# =============================================================================
def save_model_architecture_diagram(model, save_path):
    """
    Generates and stores a diagram of the model's architecture.
    """
    try:
        plot_model(
            model,
            to_file=save_path,
            show_shapes=True,
            show_layer_names=True,
            show_dtype=False,
            show_layer_activations=True,
            rankdir='TB',  # TB for a vertical layout (Top-to-Bottom)
            dpi=96
        )
        print(f"Diagram of the model's architecture stored at: {save_path}")
    except ImportError as e:
        print("\n Error while generating the diagram. Please, install the needed dependencies:")
        print("   pip install pydot graphviz")
        print("   Also, you may need to install Graphviz at a system level:")
        print("   On Debian/Ubuntu: sudo apt-get install graphviz")
        print(f"  Original error: {e}")


# =============================================================================
# MAIN FUNCTION
# =============================================================================
def main():
    """Main training pipeline"""

    # Memory and GPU info
    memory = psutil.virtual_memory()
    print(f"Available RAM: {memory.available / (1024**3):.1f} GB / {memory.total / (1024**3):.1f} GB")

    start_time = time.time()

    # Check for existing models
    existing_model, existing_label_encoder, model_count = load_existing_model()

    # Load data
    with memory_limit_check():
        segments = load_and_process_annotations(CONFIG['VIDEO_DIR'], CONFIG['ANNOTATION_FILE'])

    if len(segments) == 0:
        print("No valid segments found. Please check your data.")
        return

    # Prepare labels
    labels = [segment['label'] for segment in segments]

    if existing_label_encoder is not None:
        print("Using existing label encoder")
        label_encoder = existing_label_encoder
    else:
        print("Creating new label encoder")
        label_encoder = LabelEncoder()
        label_encoder.fit(labels)

    print(f"Class distribution:")
    unique, counts = np.unique(labels, return_counts=True)
    for class_name, count in zip(unique, counts):
        print(f"  {class_name}: {count} segments")

    NUM_CLASSES = len(label_encoder.classes_)
    print(f"Number of classes: {NUM_CLASSES}")

    # Stratified split with proper proportions
    train_segments, temp_segments = train_test_split(
        segments, test_size=CONFIG['TEST_SIZE'] + CONFIG['VAL_SIZE'],
        random_state=42, stratify=labels
    )

    temp_labels = [segment['label'] for segment in temp_segments]
    val_segments, test_segments = train_test_split(
        temp_segments, test_size=CONFIG['TEST_SIZE']/(CONFIG['TEST_SIZE'] + CONFIG['VAL_SIZE']),
        random_state=42, stratify=temp_labels
    )

    print(f"Data split:")
    print(f"  Training: {len(train_segments)} segments")
    print(f"  Validation: {len(val_segments)} segments")
    print(f"  Test: {len(test_segments)} segments")

    # Create data generators
    print("Creating data generators...")
    train_generator = VideoDataGenerator(
        train_segments, label_encoder, CONFIG,
        is_training=True, augment=True, shuffle=True
    )
    val_generator = VideoDataGenerator(
        val_segments, label_encoder, CONFIG,
        is_training=False, augment=False, shuffle=False
    )
    test_generator = VideoDataGenerator(
        test_segments, label_encoder, CONFIG,
        is_training=False, augment=False, shuffle=False
    )

    # Build or load model
    if existing_model is not None:
        print(f"Using existing model (model {model_count})")
        model = existing_model
    else:
        print("Building new 3D CNN model...")
        with memory_limit_check():
            model = build_3dcnn_model()

    print(f"Model summary:")
    model.summary()

    architecture_path = os.path.join(CONFIG['RESULTS_DIR'], 'model_architecture.png')
    save_model_architecture_diagram(model, architecture_path)

    # Calculate training parameters
    steps_per_epoch = len(train_generator)
    validation_steps = len(val_generator)

    print(f"  Training configuration:")
    print(f"  Steps per epoch: {steps_per_epoch}")
    print(f"  Validation steps: {validation_steps}")
    print(f"  Total epochs: {CONFIG['EPOCHS']}")
    print(f"  Batch size: {CONFIG['BATCH_SIZE']}")

    if existing_model is not None:
        print(f"  Resuming from existing model {model_count}")

    # Create callbacks
    callbacks = create_callbacks()

    # Train model
    print("Starting training...")
    try:
        history = model.fit(
            train_generator,
            epochs=CONFIG['EPOCHS'],
            validation_data=val_generator,
            steps_per_epoch=steps_per_epoch,
            validation_steps=validation_steps,
            callbacks=callbacks,
            verbose=1,
        )

        # Plot training history
        history_path = os.path.join(CONFIG['RESULTS_DIR'], 'training_history.png')
        plot_training_history(history, history_path)

        # Evaluate model
        print("Starting model evaluation...")
        report, cm = evaluate_model(model, test_generator, label_encoder)
        generate_qualitative_analysis_from_model(model, test_generator, label_encoder)

        # Save final model
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_model_path = os.path.join(CONFIG['CHECKPOINT_DIR'], f'final_model_{timestamp}.keras')
        model.save(final_model_path)

        # Save label encoder
        encoder_path = os.path.join(CONFIG['CHECKPOINT_DIR'], f'label_encoder_{timestamp}.pkl')
        with open(encoder_path, 'wb') as f:
            pickle.dump(label_encoder, f)

        print(f"Training completed successfully!")
        print(f"Model saved to: {final_model_path}")
        print(f"Label encoder saved to: {encoder_path}")
        print(f"Results saved to: {CONFIG['RESULTS_DIR']}")

    except Exception as e:
        print(f"Training failed with error: {e}")
        raise e
    finally:
        # Cleanup
        gc.collect()
        tf.keras.backend.clear_session()

    total_time = time.time() - start_time
    print(f"Total training time: {total_time/3600:.2f} hours")

if __name__ == "__main__":
    # GPU configuration
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"GPU acceleration enabled: {len(gpus)} GPU(s) detected")
        except RuntimeError as e:
            print(f"GPU configuration error: {e}")
    else:
        print("No GPU detected. Training will be slow.")

    main()